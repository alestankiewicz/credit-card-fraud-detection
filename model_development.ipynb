{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project goal\n",
    "\n",
    "This notebook explores various ways of detecting whether a transaction is fraudulent. The goal is to build a machine learning model that detects frauds accurately and minimizes false postives. To achive that it is crucial to handle data imbalance which is common in fraud detection problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ola\\Documents\\Projekty\\credit_card_fraud_detection\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import shap\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was sourced from Kaggle: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data\n",
    "\n",
    "This creditcard.csv is made available under the Open Database License: http://opendatacommons.org/licenses/odbl/1.0/. Any rights in individual contents of the database are licensed under the Database Contents License: http://opendatacommons.org/licenses/dbcl/1.0/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_scaled = pd.read_parquet(\"input/credit_card_scaled.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we create a sub-Sample?\n",
    "In the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe will cause the following issues:\n",
    "\n",
    "Overfitting: Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs.\n",
    "Wrong Correlations: Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features.\n",
    "Subsampling a training set, either undersampling or oversampling the appropriate class or classes, can be a helpful approach to dealing with classification data where one or more classes occur very infrequently. In such a situation (without compensating for it), most models will overfit to the majority class and produce very good statistics for the class containing the frequently occurring classes while the minority classes have poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random under-sample and SMOTE and Cost-Sensitive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest (RF) and Decision Trees - they can handle imbalanced data because of their inherent ability to find decision boundaries that separate classes well.\n",
    "Ensemble methods like AdaBoost, rfoost, LightGBM, or CatBoost, Bagging, or Stacking with base learners that handle imbalanced data well can be quite effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection is a specialized technique for handling data imbalance in\n",
    "machine learning, particularly when one class (the anomaly or rare event) is\n",
    "vastly outnumbered by the other class (normal or majority class).\n",
    "K-Means Clustering, Isolation Forest is a tree-based method that isolates anomalies by creating a random forest of decision trees. Anomalies\n",
    "are expected to require fewer splits to be isolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random under-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = credit_card_scaled.drop(columns=[\"Class\"])\n",
    "y = credit_card_scaled[\"Class\"]\n",
    "\n",
    "# Train (80%) and Test (20%) from original data set before undersampling \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=11, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227845, 30) (56962, 30)\n",
      "Class\n",
      "0    227451\n",
      "1       394\n",
      "Name: count, dtype: int64\n",
      "Class\n",
      "0    0.998271\n",
      "1    0.001729\n",
      "Name: count, dtype: float64\n",
      "Class\n",
      "0    56864\n",
      "1       98\n",
      "Name: count, dtype: int64\n",
      "Class\n",
      "0    0.99828\n",
      "1    0.00172\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.value_counts())\n",
    "print(y_train.value_counts()/y_train.count())\n",
    "print(y_test.value_counts())\n",
    "print(y_test.value_counts()/y_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frauds = credit_card_scaled[credit_card_scaled.Class == 1]\n",
    "no_frauds = credit_card_scaled[credit_card_scaled.Class == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "1    492\n",
       "0    492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undersample_df = pd.concat([frauds, no_frauds.sample(n=len(frauds), random_state=11)])\n",
    "undersample_df[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling data\n",
    "undersample_df = undersample_df.sample(frac=1, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"average_precision\" scoring metric (also known as area under the precision-recall curve) is particularly well-suited for fraud detection because:\n",
    "\n",
    "It focuses specifically on the positive class (fraud) performance\n",
    "It evaluates the model across different classification thresholds\n",
    "It isn't influenced by the large number of true negatives that dominate in highly imbalanced datasets\n",
    "\n",
    "This metric will help you find model parameters that maximize your ability to detect fraud cases while minimizing false positives, which is exactly what you want for this type of problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_randomized_search_cv(n_iter, estimator, search_space, cv, n_jobs, scoring, X_train, y_train, X_test):\n",
    "\n",
    "    search = RandomizedSearchCV(n_iter=n_iter, estimator= estimator, param_distributions=search_space,\n",
    "                              cv= cv, verbose= 2, n_jobs= n_jobs, scoring= scoring)\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "    best_model = search.best_estimator_\n",
    "    \n",
    "    preds_train = best_model.predict(X_train)\n",
    "    preds_test = best_model.predict(X_test)\n",
    "    \n",
    "    return best_model, preds_train, preds_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "# X and y for undersample: \n",
    "X_us = undersample_df.drop(columns=[\"Class\"]) \n",
    "y_us = undersample_df[\"Class\"]\n",
    "\n",
    "X_train_us, X_test_us, y_train_us, y_test_us = train_test_split(X_us, y_us, test_size=0.3, random_state= 11, stratify=y_us)\n",
    "\n",
    "rf_search_space = {\n",
    "    'n_estimators': range(10, 101),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(2, 51),\n",
    "    'min_samples_split': range(2, 11),\n",
    "    'min_samples_leaf': range(1, 11),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "xgb_search_space = {\n",
    "    \"max_depth\": randint(3, 10),\n",
    "    \"learning_rate\": uniform(0.01, 0.1),\n",
    "    \"n_estimators\": randint(100, 1000),\n",
    "    \"subsample\": uniform(0.5, 0.5),\n",
    "    \"colsample_bytree\": uniform(0.5, 0.5),\n",
    "}\n",
    "    \n",
    "# We will be testing on original X_test, not undersampled X_test_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_rf_us, preds_train_rf_us, preds_test_rf_us = best_model_randomized_search_cv(n_iter=50, estimator=RandomForestClassifier(), search_space=rf_search_space,\n",
    "                              cv=skf, n_jobs=-1, scoring=\"average_precision\", X_train=X_train_us, y_train=y_train_us, X_test=X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results for Random Forest model with undersampling on original test: \")\n",
    "print(classification_report(y_test, preds_test_rf_us))\n",
    "plt = ConfusionMatrixDisplay(confusion_matrix(y_test, preds_test_rf_us))\n",
    "plt.plot()\n",
    "print(average_precision_score(y_train_us, preds_train_rf_us))\n",
    "print(average_precision_score(y_test, preds_test_rf_us))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_xgb_us, preds_train_xgb_us, preds_test_xgb_us = best_model_randomized_search_cv(n_iter=50, estimator=xgb.XGBClassifier(), search_space=xgb_search_space,\n",
    "                              cv=skf, n_jobs=-1, scoring=\"average_precision\", X_train=X_train_us, y_train=y_train_us, X_test=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results for XGBoost model with undersampling on original test: \")\n",
    "print(classification_report(y_test, preds_test_xgb_us))\n",
    "plt = ConfusionMatrixDisplay(confusion_matrix(y_test, preds_test_xgb_us))\n",
    "plt.plot()\n",
    "print(average_precision_score(y_train_us, preds_train_xgb_us))\n",
    "print(average_precision_score(y_test, preds_test_xgb_us))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will be using X, y from original data and only applying SMOTE on training data\n",
    "xgb_search_space_smote = {\n",
    "    \"model__max_depth\": randint(3, 10),\n",
    "    \"model__learning_rate\": uniform(0.01, 0.1),\n",
    "    \"model__n_estimators\": randint(100, 1000),\n",
    "    \"model__subsample\": uniform(0.5, 0.5),\n",
    "    \"model__colsample_bytree\": uniform(0.5, 0.5),\n",
    "}\n",
    "   \n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "pipeline_xgb = Pipeline([('over', SMOTE()), ('model', xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0,\n",
    "                                                                        n_jobs=1\n",
    "                                                                        ))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_xgb_sm, preds_train_xgb_sm, preds_test_xgb_sm = best_model_randomized_search_cv(n_iter=20, estimator=pipeline_xgb, search_space=xgb_search_space_smote,\n",
    "                              cv=skf, n_jobs=-1, scoring=\"average_precision\", X_train=X_train, y_train=y_train, X_test=X_test)\n",
    "# n_jobs=-1 14min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results for XGBoost model with SMOTE on original test: \")\n",
    "print(classification_report(y_test, preds_test_xgb_sm))\n",
    "plt = ConfusionMatrixDisplay(confusion_matrix(y_test, preds_test_xgb_sm))\n",
    "plt.plot()\n",
    "print(average_precision_score(y_train, preds_train_xgb_sm))\n",
    "print(average_precision_score(y_test, preds_test_xgb_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    tree = best_model_rf_us.estimators_[i]\n",
    "    dot_data = export_graphviz(\n",
    "        tree,\n",
    "        feature_names=X_train.columns,\n",
    "        filled=True,\n",
    "        max_depth=2,\n",
    "        impurity=False,\n",
    "        proportion=True,\n",
    "    )\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: rapids for gpu accelerated random forest\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "sm = SMOTE()\n",
    "\n",
    "# pipeline_rf = Pipeline([('over', SMOTE()), ('model', RandomForestClassifier())]) \n",
    "# za ciezkie dla mojego kompa :(\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "rf_search_space = {\n",
    "    'n_estimators': range(10, 101),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(2, 51),\n",
    "    'min_samples_split': range(2, 11),\n",
    "    'min_samples_leaf': range(1, 11),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "# rf_search_space = {\n",
    "#     'model__n_estimators': range(10, 101),\n",
    "#     'model__criterion': ['gini', 'entropy'],\n",
    "#     'model__max_depth': range(2, 51),\n",
    "#     'model__min_samples_split': range(2, 11),\n",
    "#     'model__min_samples_leaf': range(1, 11),\n",
    "#     'model__max_features': ['sqrt', 'log2', None],\n",
    "#     'model__bootstrap': [True, False]\n",
    "# }\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "random_search_smote_rf = RandomizedSearchCV(n_iter=10, \n",
    "                                            # estimator= pipeline_rf,\n",
    "                                            estimator= rf_model,\n",
    "                                             param_distributions=rf_search_space,\n",
    "                              cv= skf, verbose= 3, n_jobs= -1, scoring=\"f1\")\n",
    "\n",
    "random_search_smote_rf.fit(X=X_train_smote, y=y_train_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_smote_rf = random_search_smote_rf.best_params_\n",
    "best_score_train_smote = random_search_smote_rf.best_score_\n",
    "\n",
    "best_rf_smote = RandomForestClassifier(**params_smote_rf)\n",
    "best_rf_smote.fit(X_train_smote, y_train_smote)\n",
    "pred_rf_smote = best_rf_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost-Sensitive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
